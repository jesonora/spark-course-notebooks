{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3041e2bb",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    "    <img src=\"../img/spark.png\" width=\"500\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca9aab",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a3cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.float_format', lambda x : '{:,.2f}'.format(x))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc5ab2",
   "metadata": {},
   "source": [
    "### Spark\n",
    "<img src=\"../img/spark-cluster-overview.png\" />\n",
    "\n",
    "### SparkSession vs SparkContext\n",
    "\n",
    "1. __Spark SparkContext__ is an entry point to Spark and defined in org.apache.spark package since 1.x and used to programmatically create Spark RDD, accumulators and broadcast variables on the cluster. Since Spark 2.0 most of the functionalities (methods) available in SparkContext are also available in SparkSession. Its object sc is default available in spark-shell and it can be programmatically created using SparkContext class.\n",
    "\n",
    "\n",
    "2. __SparkSession__ introduced in version 2.0 and and is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, __DataFrame__ and __DataSet__. Itâ€™s object spark is default available in spark-shell and it can be created programmatically using SparkSession builder pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a507926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/11 14:25:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "if not ('sc' in locals() or 'sc' in globals()):\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.set('spark.executor.memory', '512m')\n",
    "    conf.setMaster('spark://spark-master:7077')\n",
    "    conf.set('spark.app.name', 'template')\n",
    "\n",
    "\n",
    "    sc = SparkContext.getOrCreate(SparkContext(conf=conf))\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61973fcd",
   "metadata": {},
   "source": [
    "### Spark Running modes\n",
    "\n",
    "You could run __Spark__ in three modes basically.\n",
    "\n",
    "1. Local Mode using the local cores.\n",
    "    ```python\n",
    "    conf.setMaster('local[*]')\n",
    "    ```\n",
    "2. Using a cluster in Standalones mode.\n",
    "    ```python\n",
    "    conf.set('spark.executor.memory', '512m')\n",
    "    conf.setMaster('spark://spark-master:7077')\n",
    "    ```\n",
    "3. Using Yarn Mode (advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db629d62",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "1. __Master__\n",
    "    Web UI for the master. In this webpage we could see the different applications runnung. http://localhost:8080/\n",
    "\n",
    "\n",
    "2. __Worker__\n",
    "    Once we have any application running, we could see we jobs of the worker in this UI. http://localhost:8081/\n",
    "\n",
    "\n",
    "3. __SparkUI__\n",
    "    This UI is associated to the jupyter. Here we could see the jobs and stages of the __Spark__ application http://localhost:4040/jobs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87b036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
